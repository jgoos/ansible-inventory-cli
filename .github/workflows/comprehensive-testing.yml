name: Comprehensive Testing Suite

on:
  push:
    branches: [ main, develop, 'feature/*' ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  COVERAGE_THRESHOLD: 80

jobs:
  # Code Quality and Linting
  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
    
    - name: Run Black (formatting check)
      run: black --check scripts/
    
    - name: Run isort (import sorting check)
      run: isort --check-only scripts/
    
    - name: Run flake8 (linting)
      run: flake8 scripts/
    
    - name: Run mypy (type checking)
      run: mypy scripts/
    
    - name: Run ruff (additional linting)
      run: ruff check scripts/
    
    - name: Check documentation spelling
      run: cspell --no-progress "**/*.md"
    
    - name: Check documentation style
      run: vale --minAlertLevel=error .

  # Security Testing
  security:
    name: Security Testing
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
    
    - name: Run Bandit (security linting)
      run: |
        bandit -r scripts/ -f json -o security-report.json
        bandit -r scripts/ -f txt
    
    - name: Run Safety (dependency security check)
      run: safety check --json --output safety-report.json || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          security-report.json
          safety-report.json

  # Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        exclude:
          # Reduce matrix size for faster builds
          - os: windows-latest
            python-version: '3.8'
          - os: windows-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.8'
          - os: macos-latest
            python-version: '3.9'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
    
    - name: Run unit tests
      run: |
        pytest tests/ -m "unit" -v --tb=short \
          --cov=scripts \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --cov-fail-under=${{ env.COVERAGE_THRESHOLD }}
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == env.PYTHON_VERSION
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
    
    - name: Upload coverage HTML
      uses: actions/upload-artifact@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == env.PYTHON_VERSION
      with:
        name: coverage-html
        path: htmlcov/

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
    
    - name: Install Ansible for integration tests
      run: |
        pip install ansible-core>=2.12
        ansible --version
    
    - name: Run integration tests
      run: |
        pytest tests/ -m "integration" -v --tb=short \
          --cov=scripts \
          --cov-report=xml \
          --cov-append
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          pytest-results.xml
          coverage.xml

  # End-to-End Tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
    
    - name: Install Ansible
      run: |
        pip install ansible-core>=2.12
        ansible --version
    
    - name: Run E2E tests
      run: |
        pytest tests/ -m "e2e" -v --tb=short \
          --cov=scripts \
          --cov-report=xml \
          --cov-append
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          pytest-results.xml
          coverage.xml

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
        pip install memory-profiler
    
    - name: Run performance tests
      run: |
        pytest tests/test_performance.py -v --tb=short \
          --benchmark-only \
          --benchmark-json=benchmark-results.json
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: benchmark-results.json

  # Compatibility Tests
  compatibility-tests:
    name: Compatibility Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        ansible-version: ['2.12', '2.13', '2.14', '2.15']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
    
    - name: Install Ansible ${{ matrix.ansible-version }}
      run: |
        pip install "ansible-core>=${{ matrix.ansible-version }},<${{ matrix.ansible-version }}.99"
        ansible --version
    
    - name: Run compatibility tests
      run: |
        pytest tests/ -k "ansible" -v --tb=short
    
    - name: Test inventory generation
      run: |
        python scripts/ansible_inventory_cli.py generate --dry-run
        python scripts/ansible_inventory_cli.py validate
        python scripts/ansible_inventory_cli.py health

  # Container Tests (if using containers)
  container-tests:
    name: Container Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Podman
      run: |
        sudo apt-get update
        sudo apt-get install -y podman
    
    - name: Test with Podman
      run: |
        # Test Makefile targets that use Podman
        make --dry-run help
        # Add actual container tests if containers are used

  # Documentation Tests
  documentation-tests:
    name: Documentation Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test,docs]"
    
    - name: Test documentation examples
      run: |
        # Test that all code examples in documentation work
        python -m doctest docs/*.md || true
    
    - name: Check documentation links
      run: |
        # Check for broken links in documentation
        find docs/ -name "*.md" -exec grep -l "http" {} \; | head -5
    
    - name: Validate README examples
      run: |
        # Test examples from README
        python scripts/ansible_inventory_cli.py --help
        python scripts/ansible_inventory_cli.py health --help

  # Test Report Generation
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate comprehensive test report
      run: |
        python -c "
        import json
        import os
        from pathlib import Path
        
        report = {
            'timestamp': '$(date -u +%Y-%m-%dT%H:%M:%SZ)',
            'commit': '${{ github.sha }}',
            'branch': '${{ github.ref_name }}',
            'tests': {},
            'coverage': {},
            'security': {},
            'performance': {}
        }
        
        # Collect test results
        if Path('coverage-html').exists():
            report['coverage']['html_available'] = True
        
        if Path('security-reports').exists():
            report['security']['reports_available'] = True
        
        if Path('performance-results').exists():
            report['performance']['benchmarks_available'] = True
        
        # Save report
        with open('test-report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print('Test Report Generated:')
        print(json.dumps(report, indent=2))
        "
    
    - name: Upload test report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-report
        path: test-report.json

  # Deployment Tests (if applicable)
  deployment-tests:
    name: Deployment Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev,test]"
        pip install build twine
    
    - name: Build package
      run: |
        python -m build
    
    - name: Test package installation
      run: |
        pip install dist/*.whl
        ansible-inventory-cli --help
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: build-artifacts
        path: dist/

  # Notification
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [test-report]
    if: always()
    
    steps:
    - name: Notify on success
      if: ${{ needs.test-report.result == 'success' }}
      run: |
        echo "✅ All tests passed successfully!"
        echo "Coverage threshold: ${{ env.COVERAGE_THRESHOLD }}%"
        echo "Commit: ${{ github.sha }}"
        echo "Branch: ${{ github.ref_name }}"
    
    - name: Notify on failure
      if: ${{ needs.test-report.result != 'success' }}
      run: |
        echo "❌ Some tests failed!"
        echo "Check the test results for details."
        echo "Commit: ${{ github.sha }}"
        echo "Branch: ${{ github.ref_name }}" 